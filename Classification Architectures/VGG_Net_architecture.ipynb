{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ps1109\\AppData\\Local\\Continuum\\Anaconda3\\envs\\torch\\lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.16) or chardet (5.2.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "#Online Implementation : https://www.youtube.com/watch?v=ACmuBbuXn20&list=PLhhyoLH6IjfxeoooqP9rhU3HJIAVAJ3Vz&index=17\n",
    "# Original Source: Aladdin Persson\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG_types = {\n",
    "    \"VGG11\": [64, \"M\", 128, \"M\", 256, 256, \"M\", 512, 512, \"M\", 512, 512, \"M\", 512, 512,\"M\"],\n",
    "    \"VGG13\": [64, 64, \"M\", 128, 128, \"M\", 256, 256, \"M\", 512, 512, \"M\", 512, 512, \"M\", 512, 512,\"M\"],\n",
    "    \"VGG16\": [64, 64, \"M\", 128, 128, \"M\", 256, 256, 256, \"M\", 512, 512, 512, \"M\", 512, 512, 512, \"M\", 512, 512, 512, \"M\"],\n",
    "    \"VGG19\": [64, 64, \"M\", 128, 128, \"M\", 256, 256, 256, 256, \"M\", 512, 512, 512, 512, \"M\", 512, 512, 512, 512, \"M\", 512, 512, 512, 512, \"M\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG_net(nn.Module):\n",
    "    \"\"\"\n",
    "    VGG network implementation.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int, optional): Number of input channels (default: 3).\n",
    "        num_classes (int, optional): Number of output classes (default: 1000).\n",
    "\n",
    "    Attributes:\n",
    "        in_channels (int): Number of input channels.\n",
    "        conv_layers (Sequential): Convolutional layers of the VGG network.\n",
    "        fcs (Sequential): Fully connected layers of the VGG network.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels = 3, num_classes = 1000):\n",
    "        super(VGG_net, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.conv_layers = self.create_conv_layers(VGG_types[\"VGG16\"])\n",
    "        \n",
    "        self.fcs = nn.Sequential(nn.Linear(4608, 4096), \n",
    "                                 nn.ReLU(), \n",
    "                                 nn.Dropout(),\n",
    "                                 nn.Linear(4096, 4096, ),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Dropout(),\n",
    "                                 nn.Linear(4096, num_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the VGG network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor.\n",
    "        \"\"\"\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fcs(x)\n",
    "        return x\n",
    "    \n",
    "    def create_conv_layers(self, architecture):\n",
    "        \"\"\"\n",
    "        Create convolutional layers based on the specified architecture.\n",
    "\n",
    "        Args:\n",
    "            architecture (list): List of integers and 'M' representing layer configurations.\n",
    "\n",
    "        Returns:\n",
    "            Sequential: Sequential module containing convolutional layers.\n",
    "        \"\"\"\n",
    "        layers = []\n",
    "        in_channels =self.in_channels\n",
    "        \n",
    "        for x in architecture:\n",
    "            if type(x)== int:\n",
    "                out_channels = x \n",
    "                \n",
    "                layers += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels, \n",
    "                                     kernel_size=(3,3), stride = (1,1), padding=(1,1)),\n",
    "                            nn.BatchNorm2d(x),\n",
    "                            nn.ReLU()\n",
    "                            ]\n",
    "                in_channels = x \n",
    "            elif x == \"M\":\n",
    "                layers +=[nn.MaxPool2d(kernel_size=(2,2), stride = (2,2))]\n",
    "        return nn.Sequential(*layers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = VGG_net(in_channels=3, num_classes=1000).to(device)\n",
    "x = torch.randn(1,3, 224, 224).to(device)\n",
    "print(model(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob \n",
    "import pandas as pd \n",
    "\n",
    "path_to_data = os.path.join(\"..\", \"cifar_data\", \"cifar-10-batches-py\")\n",
    "train_df =  pd.read_csv(os.path.join(path_to_data, \"train.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(path_to_data, \"test.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6    5000\n",
       "9    5000\n",
       "4    5000\n",
       "1    5000\n",
       "2    5000\n",
       "7    5000\n",
       "8    5000\n",
       "3    5000\n",
       "5    5000\n",
       "0    5000\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    1000\n",
       "8    1000\n",
       "0    1000\n",
       "6    1000\n",
       "1    1000\n",
       "9    1000\n",
       "5    1000\n",
       "7    1000\n",
       "4    1000\n",
       "2    1000\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from numpy import array\n",
    "from pandas import DataFrame\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "from numpy import array\n",
    "from pandas import DataFrame\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "def get_from_df_paths_targets( df: DataFrame, transform=None):\n",
    "    \"\"\"\n",
    "    Extracts image paths and labels from a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The DataFrame containing image paths and labels.\n",
    "        transform (callable, optional): A transformation function to apply to images. Default is None.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The original DataFrame.\n",
    "        list: A list of image paths.\n",
    "        list: A list of labels.\n",
    "        callable: The transformation function.\n",
    "    \"\"\"\n",
    "    paths = df['path'].to_list()\n",
    "    labels = df['label'].to_list()\n",
    "    return df,paths, labels, transform\n",
    "\n",
    "class CV2ImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for handling images using OpenCV and transformations.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The DataFrame containing image paths and labels.\n",
    "        transform (callable, optional): A transformation function to apply to images. Default is None.\n",
    "        device (torch.device, optional): The target device for data tensors. Default is CUDA if available, otherwise CPU.\n",
    "\n",
    "    Attributes:\n",
    "        df (DataFrame): The DataFrame containing image paths and labels.\n",
    "        paths (list): A list of image paths.\n",
    "        labels (list): A list of labels.\n",
    "        transform (callable): The transformation function.\n",
    "        device (torch.device): The target device.\n",
    "    \"\"\"\n",
    "    def __init__(self, df:DataFrame, transform: transforms =None, device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")):\n",
    "        self.df, self.paths, self.labels, self.transform = get_from_df_paths_targets( df, transform=transform)\n",
    "        self.device=device\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get an image and its label from the dataset.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the image in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            tensor: The transformed image.\n",
    "            tensor: The label tensor.\n",
    "        \"\"\"\n",
    "        image = cv2.imread(self.paths[idx])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        label = torch.tensor(int(self.labels[idx]))\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image=image)[\"image\"]\n",
    "        return image, label\n",
    "    \n",
    "    \n",
    "class DatasetLoader():\n",
    "    \"\"\"\n",
    "    Data loader object for handling DataLoader creation.\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): The dataset to be loaded.\n",
    "        batch_size (int, optional): The batch size for each batch. Default is 1.\n",
    "        num_workers (int, optional): The number of data loading workers. Default is 1.\n",
    "\n",
    "    Attributes:\n",
    "        dataset (Dataset): The dataset to be loaded.\n",
    "        batch_size (int): The batch size for each batch.\n",
    "        num_workers (int): The number of data loading workers.\n",
    "        loader (DataLoader): The DataLoader instance.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset: Dataset, batch_size:int = 1 , num_workers: int =1):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        #self.get_dataloader()\n",
    "        \n",
    "    def get_dataloader(self,):\n",
    "        \"\"\"\n",
    "        Get a DataLoader instance for the dataset.\n",
    "\n",
    "        Returns:\n",
    "            DataLoader: The DataLoader instance.\n",
    "        \"\"\"\n",
    "        self.loader = DataLoader(\n",
    "                    self.dataset,\n",
    "                    batch_size=self.batch_size,\n",
    "                    num_workers=self.num_workers,\n",
    "                    shuffle=True\n",
    "                )\n",
    "        return self.loader\n",
    "    \n",
    "    def check_dataloader_dimension(self):\n",
    "        \"\"\"\n",
    "        Print the dimensions and types of the first batch in the DataLoader.\n",
    "        \"\"\"\n",
    "        for _, (data, target) in enumerate(self.loader):\n",
    "            print('Data Shape of Dataloader is (data, target) : ', data.shape, target.shape)\n",
    "            print('Data Type of Dataloader is (data, target) : ', type(data), type(target))\n",
    "            torch.cuda.empty_cache()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "batch_size = 4\n",
    "aug = A.Compose([   \n",
    "A.Resize(224, 224),\n",
    "A.HorizontalFlip(p=0.5),          \n",
    "A.Normalize(),            \n",
    "ToTensorV2()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CV2ImageDataset(train_df, transform=aug)\n",
    "test_dataset = CV2ImageDataset(test_df, transform=aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DatasetLoader(train_dataset, batch_size, num_workers=0).get_dataloader()\n",
    "test_loader = DatasetLoader(test_dataset, batch_size, num_workers=0).get_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CyclicLR\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_epochs = 10\n",
    "model  = VGG_net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.1)\n",
    "scheduler = scheduler = CyclicLR(optimizer, base_lr=0.001, max_lr=0.1, step_size_up=2000)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.6195, Accuracy: 0.4298\n",
      "Test Set - Loss: 1.0727, Accuracy: 0.6326\n",
      "Epoch [2/10], Loss: 0.9281, Accuracy: 0.6812\n",
      "Test Set - Loss: 0.7928, Accuracy: 0.7322\n",
      "Epoch [3/10], Loss: 0.6951, Accuracy: 0.7612\n",
      "Test Set - Loss: 0.6066, Accuracy: 0.7982\n",
      "Epoch [4/10], Loss: 0.5798, Accuracy: 0.8038\n",
      "Test Set - Loss: 0.5452, Accuracy: 0.8151\n",
      "Epoch [5/10], Loss: 0.4985, Accuracy: 0.8320\n",
      "Test Set - Loss: 0.4948, Accuracy: 0.8353\n",
      "Epoch [6/10], Loss: 0.4299, Accuracy: 0.8551\n",
      "Test Set - Loss: 0.4860, Accuracy: 0.8399\n",
      "Epoch [7/10], Loss: 0.3773, Accuracy: 0.8704\n",
      "Test Set - Loss: 0.4603, Accuracy: 0.8485\n",
      "Epoch [8/10], Loss: 0.3379, Accuracy: 0.8847\n",
      "Test Set - Loss: 0.4288, Accuracy: 0.8578\n",
      "Epoch [9/10], Loss: 0.3011, Accuracy: 0.8970\n",
      "Test Set - Loss: 0.4495, Accuracy: 0.8543\n",
      "Epoch [10/10], Loss: 0.2624, Accuracy: 0.9108\n",
      "Test Set - Loss: 0.5050, Accuracy: 0.8459\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate epoch statistics\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    epoch_accuracy = correct_predictions / len(train_dataset)\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n",
    "\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Evaluation on the test set\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    test_loss = 0.0\n",
    "    test_correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_inputs, test_labels in test_loader:\n",
    "            test_inputs, test_labels = test_inputs.to(device), test_labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            test_outputs = model(test_inputs)\n",
    "\n",
    "            # Calculate test loss\n",
    "            test_loss += criterion(test_outputs, test_labels).item() * test_inputs.size(0)\n",
    "\n",
    "            # Calculate test accuracy\n",
    "            _, test_predicted = torch.max(test_outputs, 1)\n",
    "            test_correct_predictions += (test_predicted == test_labels).sum().item()\n",
    "\n",
    "    # Calculate test set statistics\n",
    "    test_loss /= len(test_dataset)\n",
    "    test_accuracy = test_correct_predictions / len(test_dataset)\n",
    "\n",
    "    print(f\"Test Set - Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Online Resource used : https://www.youtube.com/watch?v=DkNIBBBvcPs&list=PLhhyoLH6IjfxeoooqP9rhU3HJIAVAJ3Vz&index=19\n",
    "# Original Source: Aladdin Persson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class block(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual Block with bottleneck architecture.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input channels.\n",
    "        out_channels (int): Number of output channels.\n",
    "        identity_downsample (nn.Module, optional): A downsample operation for the identity shortcut. Default is None.\n",
    "        stride (int, optional): The stride for the convolutional layers. Default is 1.\n",
    "\n",
    "    Attributes:\n",
    "        expansion (int): Expansion factor for output channels.\n",
    "        conv1 (nn.Conv2d): 1x1 convolutional layer.\n",
    "        bn1 (nn.BatchNorm2d): Batch normalization layer after the first convolution.\n",
    "        conv2 (nn.Conv2d): 3x3 convolutional layer.\n",
    "        bn2 (nn.BatchNorm2d): Batch normalization layer after the second convolution.\n",
    "        conv3 (nn.Conv2d): 1x1 convolutional layer.\n",
    "        bn3 (nn.BatchNorm2d): Batch normalization layer after the third convolution.\n",
    "        relu (nn.ReLU): ReLU activation function.\n",
    "        identity_downsample (nn.Module, optional): Downsample operation for the identity shortcut. Default is None.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, identity_downsample=None, stride = 1):\n",
    "        \"\"\"\n",
    "        Initialize a block instance.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels.\n",
    "            out_channels (int): Number of output channels.\n",
    "            identity_downsample (nn.Module, optional): A downsample operation for the identity shortcut. Default is None.\n",
    "            stride (int, optional): The stride for the convolutional layers. Default is 1.\n",
    "        \"\"\"\n",
    "        super(block, self).__init__()\n",
    "        self.expansion = 4 \n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels*self.expansion, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels*self.expansion)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.identity_downsample = identity_downsample\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the block.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor.\n",
    "        \"\"\"\n",
    "        identity = x\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.bn3(self.conv3(x))\n",
    "        \n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "            \n",
    "        x += identity\n",
    "        x = self.relu(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    \"\"\"\n",
    "        Residual Neural Network (ResNet) architecture.\n",
    "\n",
    "        Args:\n",
    "            block (nn.Module): The residual block to be used in the network.\n",
    "            layers (list): List of integers specifying the number of residual blocks in each layer.\n",
    "            image_channels (int): Number of input channels in the image.\n",
    "            num_classes (int): Number of output classes.\n",
    "\n",
    "        Attributes:\n",
    "            in_channels (int): Number of input channels for the network.\n",
    "            conv1 (nn.Conv2d): Initial convolutional layer.\n",
    "            bn1 (nn.BatchNorm2d): Batch normalization layer after the initial convolution.\n",
    "            relu (nn.ReLU): ReLU activation function.\n",
    "            maxpool (nn.MaxPool2d): Max pooling layer.\n",
    "            layer1 (nn.Sequential): First residual layer.\n",
    "            layer2 (nn.Sequential): Second residual layer.\n",
    "            layer3 (nn.Sequential): Third residual layer.\n",
    "            layer4 (nn.Sequential): Fourth residual layer.\n",
    "            avgpool (nn.AdaptiveAvgPool2d): Adaptive average pooling layer.\n",
    "            fc (nn.Linear): Fully connected layer for classification.\n",
    "        \"\"\"\n",
    "    def __init__(self, block, layers, image_channels, num_classes):\n",
    "        \"\"\"\n",
    "        Initialize a ResNet instance.\n",
    "\n",
    "        Args:\n",
    "            block (nn.Module): The residual block to be used in the network.\n",
    "            layers (list): List of integers specifying the number of residual blocks in each layer.\n",
    "            image_channels (int): Number of input channels in the image.\n",
    "            num_classes (int): Number of output classes.\n",
    "        \"\"\"\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64 \n",
    "        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=7, stride= 2 , padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride = 2, padding = 1)\n",
    "        \n",
    "        #Resnet layers\n",
    "        self.layer1 = self._make_layer(block, layers[0], out_channels=64, stride =1 )\n",
    "        self.layer2 = self._make_layer(block, layers[1], out_channels=128, stride =2 )\n",
    "        self.layer3 = self._make_layer(block, layers[2], out_channels=256, stride =2 )\n",
    "        self.layer4 = self._make_layer(block, layers[3], out_channels=512, stride =2 )\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Linear(512*4, num_classes)\n",
    "        \n",
    "        \n",
    "    def _make_layer(self, block, num_residual_blocks, out_channels, stride):\n",
    "        \"\"\"\n",
    "        Create a residual layer with multiple residual blocks.\n",
    "\n",
    "        Args:\n",
    "            block (nn.Module): The residual block to be used in the layer.\n",
    "            num_residual_blocks (int): Number of residual blocks in the layer.\n",
    "            out_channels (int): Number of output channels for each block.\n",
    "            stride (int): The stride for the first block in the layer.\n",
    "\n",
    "        Returns:\n",
    "            nn.Sequential: A sequential container of residual blocks.\n",
    "        \"\"\"\n",
    "        identity_downsample = None \n",
    "        layers = [] \n",
    "        \n",
    "        if stride !=1 or self.in_channels !=out_channels*4:\n",
    "            identity_downsample = nn.Sequential(nn.Conv2d(self.in_channels, out_channels*4, kernel_size= 1, stride = stride), \n",
    "                                                nn.BatchNorm2d(out_channels*4)\n",
    "                                                )\n",
    "        layers.append(block(self.in_channels, out_channels, identity_downsample, stride))\n",
    "        \n",
    "        self.in_channels = out_channels*4\n",
    "        \n",
    "        for i in range(num_residual_blocks - 1):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the ResNet.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor.\n",
    "        \"\"\"\n",
    "        x = self.maxpool(self.relu(self.bn1(self.conv1(x))))\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return x \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet50(img_channels= 3, num_classes = 1000):\n",
    "    \"\"\"\n",
    "    Create a ResNet-50 model.\n",
    "\n",
    "    Args:\n",
    "        img_channels (int): Number of input channels in the image.\n",
    "        num_classes (int): Number of output classes.\n",
    "\n",
    "    Returns:\n",
    "        ResNet: ResNet-50 model.\n",
    "    \"\"\"\n",
    "    return ResNet(block, [3, 4, 6, 3], img_channels, num_classes)\n",
    "\n",
    "def ResNet101(img_channels= 3, num_classes = 1000):\n",
    "    \"\"\"\n",
    "    Create a ResNet-101 model.\n",
    "\n",
    "    Args:\n",
    "        img_channels (int): Number of input channels in the image.\n",
    "        num_classes (int): Number of output classes.\n",
    "\n",
    "    Returns:\n",
    "        ResNet: ResNet-101 model.\n",
    "    \"\"\"\n",
    "    return ResNet(block, [3, 4, 23, 3], img_channels, num_classes)\n",
    "\n",
    "def ResNet152(img_channels= 3, num_classes = 1000):\n",
    "    \"\"\"\n",
    "    Create a ResNet-152 model.\n",
    "\n",
    "    Args:\n",
    "        img_channels (int): Number of input channels in the image.\n",
    "        num_classes (int): Number of output classes.\n",
    "\n",
    "    Returns:\n",
    "        ResNet: ResNet-152 model.\n",
    "    \"\"\"\n",
    "    return ResNet(block, [3, 8, 36, 3], img_channels, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    net = ResNet152()\n",
    "    x = torch.randn(2,3,224, 224)\n",
    "    y = net(x)\n",
    "    print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1000])\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob \n",
    "import pandas as pd \n",
    "\n",
    "path_to_data = os.path.join(\"..\", \"cifar_data\", \"cifar-10-batches-py\")\n",
    "train_df =  pd.read_csv(os.path.join(path_to_data, \"train.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(path_to_data, \"test.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6    5000\n",
       "9    5000\n",
       "4    5000\n",
       "1    5000\n",
       "2    5000\n",
       "7    5000\n",
       "8    5000\n",
       "3    5000\n",
       "5    5000\n",
       "0    5000\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    1000\n",
       "8    1000\n",
       "0    1000\n",
       "6    1000\n",
       "1    1000\n",
       "9    1000\n",
       "5    1000\n",
       "7    1000\n",
       "4    1000\n",
       "2    1000\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ps1109\\AppData\\Local\\Continuum\\Anaconda3\\envs\\torch\\lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.16) or chardet (5.2.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from numpy import array\n",
    "from pandas import DataFrame\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "def get_from_df_paths_targets( df: DataFrame, transform=None):\n",
    "    \"\"\"\n",
    "    Get paths and labels from a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The DataFrame containing 'path' and 'label' columns.\n",
    "        transform (callable, optional): A function/transform to apply to the data.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The input DataFrame.\n",
    "        list: List of paths.\n",
    "        list: List of labels.\n",
    "        callable: The transform function.\n",
    "    \"\"\"\n",
    "    paths = df['path'].to_list()\n",
    "    labels = df['label'].to_list()\n",
    "    return df,paths, labels, transform\n",
    "\n",
    "class CV2ImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    OpenCV-based dataset suitable for use with albumentations.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The DataFrame containing 'path' and 'label' columns.\n",
    "        transform (callable, optional): A function/transform to apply to the data.\n",
    "        device (torch.device, optional): The device to use for tensors (default: 'cuda:0' if available, else 'cpu').\n",
    "\n",
    "    Attributes:\n",
    "        df (DataFrame): The input DataFrame.\n",
    "        paths (list): List of image file paths.\n",
    "        labels (list): List of corresponding labels.\n",
    "        transform (callable): The transform function to apply to images.\n",
    "    \"\"\"\n",
    "    def __init__(self, df:DataFrame, transform: transforms =None, device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")):\n",
    "        \"\"\"\n",
    "        Initialize a CV2ImageDataset instance.\n",
    "\n",
    "        Args:\n",
    "            df (DataFrame): DataFrame containing image paths and labels.\n",
    "            transform (callable, optional): A function/transform to apply to the image data. Default is None.\n",
    "            device (torch.device, optional): The device to use for tensor computations. Default is 'cuda:0' if available, else 'cpu'.\n",
    "        \"\"\"\n",
    "        self.df, self.paths, self.labels, self.transform = get_from_df_paths_targets( df, transform=transform)\n",
    "        self.device=device\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Get the length of the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: Number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get an item from the dataset.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the item to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Transformed image.\n",
    "            torch.Tensor: Corresponding label.\n",
    "        \"\"\"\n",
    "        image = cv2.imread(self.paths[idx])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        label = torch.tensor(int(self.labels[idx]))\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image=image)[\"image\"]\n",
    "        return image, label\n",
    "    \n",
    "    \n",
    "class DatasetLoader():\n",
    "    \"\"\"\n",
    "    Data Loader object\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset: Dataset, batch_size:int = 1 , num_workers: int =1):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        #self.get_dataloader()\n",
    "        \n",
    "    def get_dataloader(self,):\n",
    "        \"\"\"\n",
    "        Method to return dataloader\n",
    "        Returns:\n",
    "            self.loader: Dataset Loader\n",
    "        \"\"\"\n",
    "        self.loader = DataLoader(\n",
    "                    self.dataset,\n",
    "                    batch_size=self.batch_size,\n",
    "                    num_workers=self.num_workers,\n",
    "                    shuffle=True\n",
    "                )\n",
    "        return self.loader\n",
    "    \n",
    "    def check_dataloader_dimension(self):\n",
    "        \"\"\"\n",
    "        Prints out the dimension of dataloader\n",
    "        \"\"\"\n",
    "        for _, (data, target) in enumerate(self.loader):\n",
    "            print('Data Shape of Dataloader is (data, target) : ', data.shape, target.shape)\n",
    "            print('Data Type of Dataloader is (data, target) : ', type(data), type(target))\n",
    "            torch.cuda.empty_cache()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "batch_size = 64\n",
    "aug = A.Compose([   \n",
    "A.HorizontalFlip(p=0.5),          \n",
    "A.Normalize(),            \n",
    "ToTensorV2()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CV2ImageDataset(train_df, transform=aug)\n",
    "test_dataset = CV2ImageDataset(test_df, transform=aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DatasetLoader(train_dataset, batch_size, num_workers=0).get_dataloader()\n",
    "test_loader = DatasetLoader(test_dataset, batch_size, num_workers=0).get_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_epochs = 10\n",
    "model  = ResNet50().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.1)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=0.0001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 3.7283, Accuracy: 0.2152\n",
      "Test Set - Loss: 29.5375, Accuracy: 0.1025\n",
      "Epoch [2/10], Loss: 2.6077, Accuracy: 0.2505\n",
      "Test Set - Loss: 7.3345, Accuracy: 0.2664\n",
      "Epoch [3/10], Loss: 1.9609, Accuracy: 0.3651\n",
      "Test Set - Loss: 9.6813, Accuracy: 0.2046\n",
      "Epoch [4/10], Loss: 1.7181, Accuracy: 0.4137\n",
      "Test Set - Loss: 28.3599, Accuracy: 0.3164\n",
      "Epoch [5/10], Loss: 1.5673, Accuracy: 0.4505\n",
      "Test Set - Loss: 5.2233, Accuracy: 0.3805\n",
      "Epoch [6/10], Loss: 1.4521, Accuracy: 0.4843\n",
      "Test Set - Loss: 3.2469, Accuracy: 0.4179\n",
      "Epoch [7/10], Loss: 1.3625, Accuracy: 0.5128\n",
      "Test Set - Loss: 71.2285, Accuracy: 0.1378\n",
      "Epoch [8/10], Loss: 1.3673, Accuracy: 0.5082\n",
      "Test Set - Loss: 2.3914, Accuracy: 0.4897\n",
      "Epoch [9/10], Loss: 1.2682, Accuracy: 0.5412\n",
      "Test Set - Loss: 1.3386, Accuracy: 0.5361\n",
      "Epoch [10/10], Loss: 1.2313, Accuracy: 0.5559\n",
      "Test Set - Loss: 1.2925, Accuracy: 0.5497\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate epoch statistics\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    epoch_accuracy = correct_predictions / len(train_dataset)\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n",
    "\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    \n",
    "    # Evaluation on the test set\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    test_loss = 0.0\n",
    "    test_correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_inputs, test_labels in test_loader:\n",
    "            test_inputs, test_labels = test_inputs.to(device), test_labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            test_outputs = model(test_inputs)\n",
    "\n",
    "            # Calculate test loss\n",
    "            test_loss += criterion(test_outputs, test_labels).item() * test_inputs.size(0)\n",
    "\n",
    "            # Calculate test accuracy\n",
    "            _, test_predicted = torch.max(test_outputs, 1)\n",
    "            test_correct_predictions += (test_predicted == test_labels).sum().item()\n",
    "\n",
    "    # Calculate test set statistics\n",
    "    test_loss /= len(test_dataset)\n",
    "    test_accuracy = test_correct_predictions / len(test_dataset)\n",
    "\n",
    "    print(f\"Test Set - Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Online Resource used : https://www.youtube.com/watch?v=uQc4Fs7yx5I&list=PLhhyoLH6IjfxeoooqP9rhU3HJIAVAJ3Vz&index=19\n",
    "# Original Source: Aladdin Persson\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_block(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Block Module consisting of Conv2D, BatchNormalization, and ReLU activation.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input channels.\n",
    "        out_channels (int): Number of output channels.\n",
    "        **kwargs: Additional keyword arguments for the Conv2D layer.\n",
    "\n",
    "    Attributes:\n",
    "        relu (nn.ReLU): ReLU activation function.\n",
    "        conv (nn.Conv2d): Conv2D layer.\n",
    "        batchnorm (nn.BatchNorm2d): Batch Normalization layer.\n",
    "\n",
    "    Methods:\n",
    "        forward(x): Forward pass of the ConvBlock module.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(conv_block, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, **kwargs)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Apply the ConvBlock to input tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor with shape (batch_size, in_channels, height, width).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Processed feature tensor after applying ConvBlock.\n",
    "        \"\"\"\n",
    "        return self.relu(self.batchnorm(self.conv(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception_block(nn.Module):\n",
    "    \"\"\"\n",
    "    Inception Block Module consisting of multiple branches with convolutional layers.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input channels.\n",
    "        out_1x1 (int): Number of output channels for the 1x1 convolution branch.\n",
    "        red_3x3 (int): Number of intermediate channels for the 1x1 convolution before the 3x3 branch.\n",
    "        out_3x3 (int): Number of output channels for the 3x3 convolution branch.\n",
    "        red_5x5 (int): Number of intermediate channels for the 1x1 convolution before the 5x5 branch.\n",
    "        out_5x5 (int): Number of output channels for the 5x5 convolution branch.\n",
    "        out_1x1pool (int): Number of output channels for the 1x1 convolution after the pooling branch.\n",
    "\n",
    "    Attributes:\n",
    "        branch1 (conv_block): Convolutional block for the 1x1 convolution branch.\n",
    "        branch2 (nn.Sequential): Sequential module for the 1x1 and 3x3 convolution branches.\n",
    "        branch3 (nn.Sequential): Sequential module for the 1x1 and 5x5 convolution branches.\n",
    "        branch5 (nn.Sequential): Sequential module for the pooling and 1x1 convolution branches.\n",
    "\n",
    "    Methods:\n",
    "        forward(x): Forward pass of the InceptionBlock module.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_1x1, red_3x3, out_3x3, red_5x5, out_5x5, out_1x1pool):\n",
    "        super(Inception_block, self).__init__()\n",
    "        \n",
    "        self.branch1 = conv_block(in_channels, out_1x1, kernel_size = 1)\n",
    "        \n",
    "        self.branch2 = nn.Sequential(conv_block(in_channels, red_3x3, kernel_size = 1),\n",
    "                                     conv_block(red_3x3, out_3x3, kernel_size = 3, padding = 1))\n",
    "        \n",
    "        self.branch3 = nn.Sequential(conv_block(in_channels, red_5x5, kernel_size = 1),\n",
    "                                     conv_block(red_5x5, out_5x5, kernel_size = 5, padding = 2))\n",
    "        \n",
    "        self.branch5  = nn.Sequential(nn.MaxPool2d(kernel_size=3, stride = 1, padding = 1), conv_block(in_channels, out_1x1pool, kernel_size = 1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Apply the InceptionBlock to input tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor with shape (batch_size, in_channels, height, width).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Processed feature tensor after applying InceptionBlock.\n",
    "        \"\"\"\n",
    "        return torch.cat([self.branch1(x), self.branch2(x), self.branch3(x), self.branch5(x)], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogleNet(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes = 1000):\n",
    "        \"\"\"\n",
    "        GoogleNet (Inception V1) architecture.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels.\n",
    "            num_classes (int): Number of output classes. Default is 1000 for ImageNet.\n",
    "\n",
    "        Attributes:\n",
    "            conv1 (conv_block): First convolutional block.\n",
    "            maxpool1 (nn.MaxPool2d): First max-pooling layer.\n",
    "            conv2 (conv_block): Second convolutional block.\n",
    "            maxpool2 (nn.MaxPool2d): Second max-pooling layer.\n",
    "            inception3a (InceptionBlock): First Inception block in the third stage.\n",
    "            inception3b (InceptionBlock): Second Inception block in the third stage.\n",
    "            maxpool3 (nn.MaxPool2d): Third max-pooling layer.\n",
    "            inception4a (InceptionBlock): First Inception block in the fourth stage.\n",
    "            inception4b (InceptionBlock): Second Inception block in the fourth stage.\n",
    "            inception4c (InceptionBlock): Third Inception block in the fourth stage.\n",
    "            inception4d (InceptionBlock): Fourth Inception block in the fourth stage.\n",
    "            inception4e (InceptionBlock): Fifth Inception block in the fourth stage.\n",
    "            maxpool4 (nn.MaxPool2d): Fourth max-pooling layer.\n",
    "            inception5a (InceptionBlock): First Inception block in the fifth stage.\n",
    "            inception5b (InceptionBlock): Second Inception block in the fifth stage.\n",
    "            avgpool (nn.AvgPool2d): Average pooling layer.\n",
    "            dropout (nn.Dropout): Dropout layer with a dropout rate of 0.4.\n",
    "            fc1 (nn.Linear): First fully connected layer.\n",
    "            fc2 (nn.Linear): Second fully connected layer.\n",
    "\n",
    "        Methods:\n",
    "            forward(x): Forward pass of the GoogleNet model.\n",
    "\n",
    "        \"\"\"\n",
    "        super(GoogleNet, self).__init__()\n",
    "        self.conv1 = conv_block(in_channels=in_channels, out_channels= 64, kernel_size = (7,7), stride = (2,2), padding = (3,3))\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride = 2, padding =1)\n",
    "        self.conv2 = conv_block(64, 192, kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=3, stride=1, padding = 1)\n",
    "        \n",
    "        self.inception3a = Inception_block(192, 64, 96, 128, 16, 32, 32)\n",
    "        self.inception3b = Inception_block(256, 128, 128, 192, 32, 96, 64)\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=3, stride = 2, padding =1)\n",
    "        \n",
    "        self.inception4a = Inception_block(480, 192, 96, 208, 16, 48, 64)\n",
    "        self.inception4b = Inception_block(512, 160, 112, 224, 24, 64, 64)\n",
    "        self.inception4c = Inception_block(512, 128, 128, 256, 24, 64, 64)\n",
    "        self.inception4d = Inception_block(512, 112, 144, 288, 32, 64, 64)\n",
    "        self.inception4e = Inception_block(528, 256, 160, 320, 32, 128, 128)\n",
    "        self.maxpool4 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.inception5a = Inception_block(832, 256, 160, 320, 32, 128, 128)\n",
    "        self.inception5b = Inception_block(832, 384, 192, 384, 48, 128, 128)\n",
    "        \n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=7, stride = 1)\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "        self.fc1 = nn.Linear(65536, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 1000)\n",
    "        \n",
    "    def forward (self, x):\n",
    "        \"\"\"\n",
    "        Apply the GoogleNet model to input tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor with shape (batch_size, in_channels, height, width).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Predicted output tensor.\n",
    "        \"\"\"\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        \n",
    "        x = self.inception3a(x)\n",
    "        x = self.inception3b(x)\n",
    "        x = self.maxpool3(x)\n",
    "        \n",
    "        x = self.inception4a(x)\n",
    "        x = self.inception4b(x)\n",
    "        x = self.inception4c(x)\n",
    "        x = self.inception4d(x)\n",
    "        x = self.inception4e(x)\n",
    "        x = self.maxpool4(x)\n",
    "        \n",
    "        x = self.inception5a(x)\n",
    "        x = self.inception5b(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1000])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    x =torch.randn(3,3,224, 224)\n",
    "    model = GoogleNet(in_channels=3)\n",
    "    print(model(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob \n",
    "import pandas as pd \n",
    "\n",
    "path_to_data = os.path.join(\"..\", \"cifar_data\", \"cifar-10-batches-py\")\n",
    "train_df =  pd.read_csv(os.path.join(path_to_data, \"train.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(path_to_data, \"test.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6    5000\n",
       "9    5000\n",
       "4    5000\n",
       "1    5000\n",
       "2    5000\n",
       "7    5000\n",
       "8    5000\n",
       "3    5000\n",
       "5    5000\n",
       "0    5000\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    1000\n",
       "8    1000\n",
       "0    1000\n",
       "6    1000\n",
       "1    1000\n",
       "9    1000\n",
       "5    1000\n",
       "7    1000\n",
       "4    1000\n",
       "2    1000\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ps1109\\AppData\\Local\\Continuum\\Anaconda3\\envs\\torch\\lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.16) or chardet (5.2.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from numpy import array\n",
    "from pandas import DataFrame\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "def get_from_df_paths_targets( df: DataFrame, transform=None):\n",
    "    \"\"\"\n",
    "    Get paths and labels from a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input DataFrame containing 'path' and 'label' columns.\n",
    "        transform (callable, optional): A transformation to apply to the images.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: A tuple containing the DataFrame, paths, labels, and transform function.\n",
    "    \"\"\"\n",
    "    paths = df['path'].to_list()\n",
    "    labels = df['label'].to_list()\n",
    "    return df,paths, labels, transform\n",
    "\n",
    "class CV2ImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset using OpenCV and compatible with Albumentations transformations.\n",
    "    \"\"\"\n",
    "    def __init__(self, df:DataFrame, transform: transforms =None, device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")):\n",
    "        \"\"\"\n",
    "        Initialize the CV2ImageDataset.\n",
    "\n",
    "        Args:\n",
    "            df (DataFrame): The DataFrame containing image paths and labels.\n",
    "            transform (callable, optional): A transformation to apply to the images.\n",
    "            device (torch.device, optional): The device to use for data processing.\n",
    "        \"\"\"\n",
    "        self.df, self.paths, self.labels, self.transform = get_from_df_paths_targets( df, transform=transform)\n",
    "        self.device=device\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get an item from the dataset.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the item to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            Tuple: A tuple containing the image tensor and its label.\n",
    "        \"\"\"\n",
    "        image = cv2.imread(self.paths[idx])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        label = torch.tensor(int(self.labels[idx]))\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image=image)[\"image\"]\n",
    "        return image, label\n",
    "    \n",
    "    \n",
    "class DatasetLoader():\n",
    "    \"\"\"\n",
    "    Data Loader object\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset: Dataset, batch_size:int = 1 , num_workers: int =1):\n",
    "        \"\"\"\n",
    "        Initialize the DatasetLoader.\n",
    "\n",
    "        Args:\n",
    "            dataset (Dataset): The PyTorch dataset to load.\n",
    "            batch_size (int, optional): Batch size for data loading.\n",
    "            num_workers (int, optional): Number of workers for parallel data loading.\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        #self.get_dataloader()\n",
    "        \n",
    "    def get_dataloader(self,):\n",
    "        \"\"\"\n",
    "        Get a DataLoader object for the specified dataset.\n",
    "\n",
    "        Returns:\n",
    "            DataLoader: A DataLoader for the dataset.\n",
    "        \"\"\"\n",
    "        self.loader = DataLoader(\n",
    "                    self.dataset,\n",
    "                    batch_size=self.batch_size,\n",
    "                    num_workers=self.num_workers,\n",
    "                    shuffle=True\n",
    "                )\n",
    "        return self.loader\n",
    "    \n",
    "    def check_dataloader_dimension(self):\n",
    "        \"\"\"\n",
    "        Print the dimensions of the DataLoader.\n",
    "        \"\"\"\n",
    "        for _, (data, target) in enumerate(self.loader):\n",
    "            print('Data Shape of Dataloader is (data, target) : ', data.shape, target.shape)\n",
    "            print('Data Type of Dataloader is (data, target) : ', type(data), type(target))\n",
    "            torch.cuda.empty_cache()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "batch_size = 2\n",
    "aug = A.Compose([   \n",
    "A.Resize(224, 224),\n",
    "A.HorizontalFlip(p=0.5),          \n",
    "A.Normalize(),            \n",
    "ToTensorV2()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CV2ImageDataset(train_df, transform=aug)\n",
    "test_dataset = CV2ImageDataset(test_df, transform=aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DatasetLoader(train_dataset, batch_size, num_workers=0).get_dataloader()\n",
    "test_loader = DatasetLoader(test_dataset, batch_size, num_workers=0).get_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CyclicLR\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_epochs = 10\n",
    "model  = GoogleNet(3).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.1)\n",
    "scheduler = scheduler = CyclicLR(optimizer, base_lr=0.001, max_lr=0.1, step_size_up=2000)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.6781, Accuracy: 0.3960\n",
      "Test Set - Loss: 1.3949, Accuracy: 0.5081\n",
      "Epoch [2/10], Loss: 1.0622, Accuracy: 0.6249\n",
      "Test Set - Loss: 0.9395, Accuracy: 0.6789\n",
      "Epoch [3/10], Loss: 0.7980, Accuracy: 0.7227\n",
      "Test Set - Loss: 1.0016, Accuracy: 0.6940\n",
      "Epoch [4/10], Loss: 0.6557, Accuracy: 0.7770\n",
      "Test Set - Loss: 0.8398, Accuracy: 0.7389\n",
      "Epoch [5/10], Loss: 0.5580, Accuracy: 0.8086\n",
      "Test Set - Loss: 0.7331, Accuracy: 0.7672\n",
      "Epoch [6/10], Loss: 0.4902, Accuracy: 0.8320\n",
      "Test Set - Loss: 0.6158, Accuracy: 0.7986\n",
      "Epoch [7/10], Loss: 0.4412, Accuracy: 0.8499\n",
      "Test Set - Loss: 0.7511, Accuracy: 0.7886\n",
      "Epoch [8/10], Loss: 0.3960, Accuracy: 0.8651\n",
      "Test Set - Loss: 0.5356, Accuracy: 0.8296\n",
      "Epoch [9/10], Loss: 0.3561, Accuracy: 0.8797\n",
      "Test Set - Loss: 1.0990, Accuracy: 0.7557\n",
      "Epoch [10/10], Loss: 0.3295, Accuracy: 0.8859\n",
      "Test Set - Loss: 0.5161, Accuracy: 0.8423\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate epoch statistics\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    epoch_accuracy = correct_predictions / len(train_dataset)\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n",
    "\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Evaluation on the test set\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    test_loss = 0.0\n",
    "    test_correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_inputs, test_labels in test_loader:\n",
    "            test_inputs, test_labels = test_inputs.to(device), test_labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            test_outputs = model(test_inputs)\n",
    "\n",
    "            # Calculate test loss\n",
    "            test_loss += criterion(test_outputs, test_labels).item() * test_inputs.size(0)\n",
    "\n",
    "            # Calculate test accuracy\n",
    "            _, test_predicted = torch.max(test_outputs, 1)\n",
    "            test_correct_predictions += (test_predicted == test_labels).sum().item()\n",
    "\n",
    "    # Calculate test set statistics\n",
    "    test_loss /= len(test_dataset)\n",
    "    test_accuracy = test_correct_predictions / len(test_dataset)\n",
    "\n",
    "    print(f\"Test Set - Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
